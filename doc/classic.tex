For product state approximation algorithms, many techniques from classical algorithms are used and generalized.
Finding the maximal eigenvalue of a traceless 2-local hamiltonian is the quantum analogue to maximizing a binary quadratic program (MaxQP):
Given a matrix $A$ with $a_{ii}=0$ maximize \[
	\sum_{i=1}^{n}\sum_{j=1}^{n} a_{ij}x_{i}x_{j}\quad \text{s.t.}\quad x_{i}\in \{-1,1 \}\quad \forall ~ i
.\]
An important tool for solving such problems is the relaxation of a semidefinite program (SDP), which has been pioneered by Goemans and Williamson.\\
In semidefinite programing we try to maximize a linear function, such that an affine combination of symmetric matrices is positive semidefinite.
An affine combination is a linear combination $\sum_{i=1}^{n}a_ix_i$ where $x_i$ are elements of a vector space, such that $\sum_{i=1}^{n}a_i=1$.
Semidefinite programs are very useful, as they can be solved efficiently both in theory and in practice \cite{vandenberghe96}.
We can write a general SDP as:
\begin{flalign*}
	\text{minimize} &\quad C \cdot X\\
	\text{s.t.} &\quad A_i \cdot X = b_i,\quad i=1,\ldots,m\\
	                  &\quad X \ge 0
\end{flalign*}
where $C$ and $A_i$ are symmetric matrices and $b_i\in\IR^{m}$ is a vector.
This is called the primal problem.
The dual of a SDP is its reformulated version, such that instead of minimizing (maximizing) an objective function, we maximize (minimize) another:
\begin{flalign*}
	\text{maximize} &\quad \sum_{i=1}^{m} y_{i}b_i\\
	\text{s. t.} &\quad C - \sum_{i=1}^{m} y_iA_i \ge 0
\end{flalign*}
If the optimal value of the primal and the dual problem are the same, we say that \emph{strong dualitity} holds.
While this is not the case in general, it usually holds for SDPs.
For SDPs we can use Slaters condition for strong duality, which states that if there is an $x^*$ that is strictly feasible (i.e. all constraints are satisfied and inequalities hold), then the problem is strictly dual \cite{boyd04}.\\
The relaxation of a SDP was first proposed by Goemans and Williamson in \cite{goemans95} as part of an approximation algorithm for the max-cut problem, which is a special case of MaxQP.
In max-cut, we are given a graph and are to find a partition of the vertices into two sets, such that the number of edges between the two sets is as large as possible.
Goemans and Williamson start by reformulating the problem as an SDP.
Given a vertex set $ V=\{1,\ldots n\} $ and non-negative weights $w_{i,j}=w_{j,i}$, maximize the objective function $\frac{1}{2}\sum_{i<j} w_{i,j}\left( 1-y_iy_j \right)$ such that  $y_i \in S=\{-1,1\}\quad \forall ~ i \in V$.
As this is in NP, we need to relax the constraints, which is accomplished by extending the objective function to a larger space, namely $S^n= \{-1,1\}^n$.
We then have to consider vectors $v_i$ and look at the inner product  $v_i\cdot v_j$.
The algorithm proposed by Goemans and Williamson proceeds by partitioning the vertices of the graph based on randomized rounding.
The rounding is based on a random hyperplane cut of the vectors.
It has a approximation ratio of $0.878$.\\
The first algorithm for approximating an optimal solution of MaxQP was proposed by Charikar and Wirth and has a $\Omega\left( \frac{1}{\log n} \right)$ approximation ratio \cite{charikar04}.
Since the algorithm described in the next chapter uses a similar approach, I will now lay out the ideas of arriving at the classical result.
It also uses relaxation of a SDP and randomized rounding, but instead of partitioning based on a random hyperplane cut through the origin, it takes into account the size of the projections of a random vector onto the solution vectors.
For this, the relaxed semidefinite program is
\begin{flalign*}
	\text{max} &\quad \sum_{ij} a_{ij}v_{i}\cdot v_{j}\\
	\text{s.t.} &\quad  v_i\cdot v_i =1 \quad \forall ~ i\\
	            &\quad v_i\in\IR^n.
\end{flalign*}
Using this the algorithm which can be solved in polynomial time is:
\begin{enumerate}
	\item Obtain an optimal solution $ \{v_i\} $ to the SDP
	\item Create vector $r$ in which the $r_i$ are independently distributed over the normal distribution
	\item Let $z_i=v_i\cdot r /T$, where $T=\sqrt{4\log n} $
	\item If $\left| z_i \right| > 1$ then $y_i=\text{sgn}(z_i)$, otherwise $y_i=z_i$
	\item Round the $y_i$ to $\pm 1$
\end{enumerate}
Step 4 truncates any values outside $\left[ -1,+1\right] $
The last rounding step is based on the size of the $y_{i}\in\left[ -1,1\right]^{n}$: \[
x_{i}=\begin{cases}
	-1, \quad \text{with probability} \frac{1-y_{i}}{2}\\
	+1, \quad \text{with probability} \frac{1+y_{i}}{2}
\end{cases}
\]
The result then clearly lies in $\left[ -1,+1\right]$.
The proof that this algorithm indeed has an approximation ratio of $\Omega(\frac{1}{\log{}n}$ exploits the spherical symmetry of the distribution of the vector $r$ to show that \[
T^2\mathbb{E}\left[ z_iz_j \right] = v_i\cdot v_j
.\]
Therefore, if every $\left| z_i \right| $ would be at most one, the approximation ratio would be $\frac{1}{T^2}$.
Since this is not the case, we have to proof that the expected value of $\Delta_{ij}=z_iz_j - y_iy_j$ is small in magnitude.
In this case \[
	\left|\mathbb{E}(\Delta_{ij})\right|<8e^{-\frac{T^2}{2}}
.\]
The latter can be used to finally show that the $x_i$ are a good approximation to the $y_i$.
This algorithm also efficiently solves max-cut and can therefore be seen as a generalization of \cite{goemans95}.
In the next chapter, we will see the techniques used here to tackle the problem of approximating the maximal value of a traceless $2$-local Hamiltonian.\\
Others have studied the approximation algorithms for Hamiltonians which are positive semidefinite instead of traceless \cite{gharibian19,anshu20,brandao14}.
An interesting example of this case is the Heisenberg model, which has already been studied by Bethe in 1931 in \cite{bethe31}, where the famous Bethe ansatz is presented.
The Heisenberg model is a family of $2$-local Hamiltonians of the form \[
H=\sum_{i,j} w_{ij}H_{ij}
,\] where \[
H_{ij}=\alpha X_iX_i + \beta Y_iY_j + \gamma Z_iZ_j
.\]
In \cite{gharibian19}, constant approximation ratios are achieved for special cases of $\alpha, \beta, \gamma$.
For $\alpha+\beta+\gamma=3$, the problem of finding the maximal eigenvalue of an equivalent version of $H$, namely with  $H_{ij}=I-(\alpha X_iX_i + \beta Y_iY_j + \gamma Z_iZ_j)$, is equivalent to the max-cut problem.
Gharibian and Parekh demonstrated a classical approximation algorithm with an approximation ratio of at least $0.498$.
This year, Anshu \emph{et al.}\cite{anshu20} have shown an efficient classical approximation algorithm with a ratio of at least $0.53$ for the same problem, and that for specific instances of the problem there is a shallow quantum circuit that prepares a state with energy larger than the best product state and even its SDP relaxation.
The circuit is efficiently computable and shallow means that its depth is independent of input size.
